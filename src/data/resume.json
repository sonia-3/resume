{
  "basics": {
    "name": "Gnana Chaithanya Gali",
    "title": "Data analyst",
    "summary": "Passionate software engineer with 2 years of IT experience in Life sciences projects and worked in Node.js, Express js (middleware), Python, SQL, Aws cloud services as a Backend developer and Data engineer",
    "location": "Bangalore, KA (Open to Remote)",
    "image": "https://images.unsplash.com/photo-1472099645785-5658abf4ff4e?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=200&h=200&q=80",
    "social": {
      "github": "https://github.com/chaithanyagali",
      "linkedin": "https://www.linkedin.com/in/gnana-chaithanya-gali-9ab529173",
      "email": "chaithanyagali2001@gmail.com"
    }
  },
  "experience": [
    {
      "title": "Data Analyst",
      "company": "Cognizant Technology Solutions",
      "period": "Nov 2022 - present",
      "achievements": [
        "worked as a developer in life science AWS data migration for EUMDR",
        "worked on a POC for a project as a single developer",
        "Built a frame work in nodejs for a seamless backend api"
      ]
    },
    {
      "title": "Intern",
      "company": "Cognizant Technology Solutions",
      "period": "Mar 2022 - Nov 2022",
      "achievements": [
        "worked on bigdata etl(airflow , pyspark , Hadoop, HDFS and Hive)"
      ]
    }
  ],
  "skills": {
    "technical": [
      "SQL",
      "PostgreSQL",
      "Python",
      "Pyspark",
      "Node.js",
      "AWS",
      "Machine learning",
      "Step Function", 
      "Airflow",
      "redshift"
    ],
    "soft": [
      "Team Leadership",
      "Problem Solving",
      "Communication",
      "Agile",
      "Mentoring"
    ]
  },
  "projects": [
    {
      "title": "Acuvue Loyalty Dashboard",
      "client": "Johnson and Johnson",
      "tools": ["React", "D3.js", "TypeScript", "GraphQL", "PostgreSQL"],
      "role": "Backend Developer",
      "teamSize": 5,
      "model": "Agile/Scrum",
      "startDate": "July 2023",
      "endDate": "present",
      "description": "it’s a US related web application Project which helps the Doctors to calculate and get the history rebate details based on the revenue and share they generated by selling the acuvue lens. They had more than 60000 groups where each group had more than 70 accounts under them.",
      "responsibilities": [
        "Worked from the POC of this project-built an architecture to extract the data from redshift database and provide to UI based on the API call",
        "Developed AWS Lambda Using Expressjs and Nodejs to respond to API call with the required Output to be displayed",
        "Loaded the data in redshift tables from the CSV files using the Dataiku DSS",
        "Developed a new architecture to Load the data into EFS Files After the data refresh and use EFS Files to extract the Json instead of running query for every call as redshift has limited no of connections",
        "Used DynamoDB as a cache for the subsequent call instead of going to efs",
        "Built a step function to load the data from Redshift to EFS files using lambda and DynamoDB for logging"
      ],
      "link": "https://example.com/project2"
    },
    {
      "title": "EUMDR (European Union Medical Device Regulation)",
      "client": "Johnson and Johnson",
      "tools": ["AWS", "python" , "pyspark" , "Airflow"],
      "role": "Developer",
      "teamSize": 25,
      "model": "Agile/Scrum",
      "startDate": "November 2022",
      "endDate": "July 2023",
      "description": "It’s a European Data Management project relates with health care domain to create ETL pipeline for medical devices which is Data from different Sources.",
      "responsibilities": [
        "Developed ETL workflow using Apache Airflow, API Gateway, lambda, Glue and DynamoDB to automate data ingestion, transformation and loading processes",
        "Developed AWS Glue jobs for ingesting incremental data from multiple sources, performing transformation, and maintaining Hudi based tables in Data Lake.",
        "Created AWS lambda functions to handle event-based activities.",
        "Developed spark code in python using spark SQL & Datasets and RDD for batch jobs",
        "Created Apache airflow for running a scheduled data ingestion, transformation and load Data"
      ],
      "link": "https://example.com/project1"
    }
  ]
}